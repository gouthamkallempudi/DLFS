{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecb18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nn\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c40568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inpyt image size\n",
    "ISIZE = (800, 800)\n",
    "\n",
    "# Imagenet statistics\n",
    "imagenet_stats = np.array([[0.485, 0.456, 0.406] , [0.229, 0.224, 0.225]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7618938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def normalize(im):\n",
    "    # convert image to float \n",
    "    im = im / 255.\n",
    "    \"\"\" Normalize with image net stats\"\"\"\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "def train_val_dataset(dataset, val_split = 0.1):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size = val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val']   = Subset(dataset, val_idx)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad7f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set and tranforms\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all the image files, sorting them to ensure they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load image and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\" , self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize(ISIZE)\n",
    "        img = np.array(img)\n",
    "        img = normalize(img)\n",
    "        img = img.transpose(2,0,1)\n",
    "        img = torch.as_tensor(img, dtype = torch.float32)\n",
    "        \n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.resize(ISIZE)\n",
    "        mask = np.array(mask)\n",
    "        obj_ids = np.unique(mask)            # instances are encoded as different colors (0--backhroung)\n",
    "        obj_ids = obj_ids[1:]                # first id is background remove it\n",
    "        # split the color-encoded mask into a set of binary masks (i.e true or false)\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        # convert to torch tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype = torch.float32)   # box dims\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)       # true or false\n",
    "        labels = torch.ones((num_objs,) , dtype = torch.int64)  # no od persons\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])   # area\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c409054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "dataset = PennFudanDataset('./data/PennFudanPed/', None)\n",
    "\n",
    "datasets = train_val_dataset(dataset)\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(datasets['train'], batch_size = 15, shuffle = True)\n",
    "data_loader_val = torch.utils.data.DataLoader(datasets['val'], batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d71e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "is_cuda= False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print(is_cuda)\n",
    "\n",
    "model = torchvision.models.vgg16(pretrained = True)\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "fe = list(model.features)\n",
    "req_features = []\n",
    "for j, i in enumerate(fe[0:30]):\n",
    "    req_features.append(i)                          # Remove last pooling layer\n",
    "    print(i)\n",
    "    \n",
    "#print(req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de953e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(req_features, model, optimizer, train_dl, val_dl, epochs = 10, rpn_lambda = 10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()   # https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        sum_loss_cls = 0\n",
    "        sum_loss_loc = 0\n",
    "        idx = 0\n",
    "        for images, targets in train_dl:\n",
    "            idx += 1\n",
    "            num_batch = len(images)\n",
    "            print(f'batch --> {idx} , epoch --> {epoch}')\n",
    "            imgs_torch_all = torch.stack([item for item in images])\n",
    "            imgs_clone     = imgs_torch_all.clone()   # copies to a new tensor\n",
    "            \n",
    "            for feature in req_features:\n",
    "                imgs_clone = feature(imgs_clone    )\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 512, mid_channels = 512, n_anchor = 9):\n",
    "        super(RNN, self).__init__()\n",
    "        self.in_channels  = in_channels   # depends on output of feature map in vgg 16 its 512\n",
    "        self.mid_channels = mid_channels\n",
    "        self.n_anchor     = n_anchor      # no of anchors in each loacation\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, kernel_size = 3, stride = 1, padding =1)\n",
    "        self.reg_layer = nn.Conv2d(self.mid_channels, n_anchor * 4 , 1 , 1 , 0)\n",
    "        self.cls_layer = nn.Conv2d(self.mid_channels, n_anchor * 2 , 1 , 1 , 0)        # see the paper\n",
    "        \n",
    "        \n",
    "        # Conv sliding layer\n",
    "        self.conv1.weight.data.normal_(0,0.01)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        # Regression Layer\n",
    "        self.ref_layer.weight.data.normal_(0,0.01)\n",
    "        self.ref_layer.bias.data.zero_()\n",
    "        # Classification Layer = RPN\n",
    "        self.cls_layer.weight.data.normal_(0,0.01)\n",
    "        self.cls_layer.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, k): \n",
    "        bat_num = k.shape[0]\n",
    "        x = self.conv1(k)\n",
    "        pred_anchor_boxes = self.reg_layer(x)\n",
    "        pred_cls_scores   = self.cls_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d396d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0015\n",
    "num_epochs    = 20\n",
    "model = RPN()\n",
    "\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(parameters , lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_epochs(req_features, model, optimizer, data_loader, None, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2ac2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9342231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6456fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e311c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf76d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e88933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About dilation max pooling https://datascience.stackexchange.com/questions/28881/what-is-dilated-pooling-and-how-it-works-mathematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5551d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
